#!/usr/bin/env python3
"""
seofrog/cli.py
CLI Interface v0.2 - Integrado com Core Engine
"""

import argparse
import os
import sys
from typing import Dict, Any, Tuple, Optional
from urllib.parse import urlparse

# === IMPORTS INTERNOS ===
from seofrog.core.config import CrawlConfig, ProfileConfig, create_config_from_profile, create_auto_config
from seofrog.core.exceptions import ConfigException, ValidationException

# ==================== URL SANITIZER ====================

def sanitize_url(url: str) -> str:
    """Sanitiza e valida URL de entrada"""
    if not url:
        raise ValidationException("URL n√£o pode estar vazia")
    
    url = url.strip()
    
    # Remove caracteres inv√°lidos comuns
    url = url.replace(' ', '%20')
    
    # Adiciona protocolo se n√£o existir
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Valida URL b√°sica
    try:
        parsed = urlparse(url)
        if not parsed.netloc:
            raise ValidationException("URL inv√°lida - dom√≠nio n√£o encontrado", field="url", value=url)
        
        # Valida caracteres no dom√≠nio
        domain = parsed.netloc.lower()
        if any(char in domain for char in ['<', '>', '"', '`', ' ']):
            raise ValidationException("Dom√≠nio cont√©m caracteres inv√°lidos", field="domain", value=domain)
        
        return url
    except Exception as e:
        if isinstance(e, ValidationException):
            raise
        raise ValidationException(f"URL inv√°lida: {url}", field="url", value=url)

# ==================== CLI PARSER ====================

def create_cli_parser() -> argparse.ArgumentParser:
    """Cria parser CLI enterprise"""
    
    profiles = ProfileConfig.get_profiles()
    auto_workers = os.cpu_count() or 4
    
    parser = argparse.ArgumentParser(
        description='üê∏ SEOFrog v0.2 Enterprise - Professional Screaming Frog Clone',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
üöÄ EXEMPLOS DE USO:

  Crawl autom√°tico (configura√ß√£o inteligente):
    seofrog exemplo.com
    seofrog https://loja.com
    seofrog blog.com.br

  Profiles pr√©-configurados:
    seofrog exemplo.com --profile quick      # {profiles['quick'].description}
    seofrog exemplo.com --profile standard   # {profiles['standard'].description}
    seofrog exemplo.com --profile deep       # {profiles['deep'].description}
    seofrog exemplo.com --profile safe       # {profiles['safe'].description}
    seofrog exemplo.com --profile aggressive # {profiles['aggressive'].description}

  Configura√ß√£o manual (sobrescreve profiles):
    seofrog exemplo.com --max-urls 50000 --workers 16 --delay 0.1
    seofrog exemplo.com --profile deep --max-urls 1000000
    seofrog exemplo.com --no-robots --crawl-images --crawl-css

  An√°lise de resultados:
    seofrog --analyze resultado.csv
    seofrog --analyze /path/to/crawl_data.csv

üîß CONFIGURA√á√ïES AUTO-DETECTADAS:
    CPU Cores: {auto_workers}
    Workers padr√£o: {auto_workers}
    Mem√≥ria recomendada: {auto_workers * 256}MB
    
üìã PROFILES DISPON√çVEIS:
    Use --list-profiles para ver detalhes completos
        """
    )
    
    # ==================== ARGUMENTOS PRINCIPAIS ====================
    
    parser.add_argument('url', nargs='?', 
                       help='URL inicial (adiciona https:// automaticamente se necess√°rio)')
    
    # ==================== PROFILES ====================
    
    profile_group = parser.add_argument_group('üéØ Profiles Pr√©-configurados')
    profile_group.add_argument('--profile', choices=list(profiles.keys()),
                              help='Profile de crawl enterprise')
    profile_group.add_argument('--list-profiles', action='store_true',
                              help='Lista todos os profiles dispon√≠veis com detalhes')
    
    # ==================== CONFIGURA√á√ïES PRINCIPAIS ====================
    
    config_group = parser.add_argument_group('‚öôÔ∏è Configura√ß√µes de Crawl')
    config_group.add_argument('--max-urls', type=int,
                             help='M√°ximo de URLs para crawlear')
    config_group.add_argument('--max-depth', type=int,
                             help='Profundidade m√°xima do crawl')
    config_group.add_argument('--workers', type=int,
                             help=f'Threads trabalhadoras (auto: {auto_workers})')
    config_group.add_argument('--delay', type=float,
                             help='Delay entre requests em segundos')
    config_group.add_argument('--timeout', type=int,
                             help='Timeout por request em segundos')
    
    # ==================== COMPORTAMENTO ====================
    
    behavior_group = parser.add_argument_group('ü§ñ Comportamento')
    behavior_group.add_argument('--no-robots', action='store_true',
                               help='Ignora robots.txt (n√£o recomendado)')
    behavior_group.add_argument('--no-redirects', action='store_true',
                               help='N√£o segue redirects')
    behavior_group.add_argument('--max-redirects', type=int,
                               help='M√°ximo de redirects a seguir (padr√£o: 10)')
    
    # ==================== TIPOS DE CONTE√öDO ====================
    
    content_group = parser.add_argument_group('üìÅ Tipos de Conte√∫do')
    content_group.add_argument('--crawl-images', action='store_true',
                              help='Inclui imagens no crawl')
    content_group.add_argument('--crawl-css', action='store_true',
                              help='Inclui arquivos CSS')
    content_group.add_argument('--crawl-js', action='store_true',
                              help='Inclui arquivos JavaScript')
    content_group.add_argument('--crawl-pdf', action='store_true',
                              help='Inclui arquivos PDF')
    
    # ==================== REDE E PERFORMANCE ====================
    
    network_group = parser.add_argument_group('üåê Rede e Performance')
    network_group.add_argument('--user-agent',
                              help='User-Agent customizado')
    network_group.add_argument('--retry-attempts', type=int,
                              help='Tentativas de retry por URL')
    network_group.add_argument('--memory-limit', type=int,
                              help='Limite de mem√≥ria em MB')
    
    # ==================== OUTPUT ====================
    
    output_group = parser.add_argument_group('üíæ Output e Export')
    output_group.add_argument('--output', 
                             help='Diret√≥rio de output')
    output_group.add_argument('--format', choices=['csv', 'xlsx'], 
                             help=f'Formato de export (default: xlsx)')
    output_group.add_argument('--filename', 
                             help='Nome do arquivo de output')
    output_group.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                             help='N√≠vel de logging')
    
    # ==================== UTILIT√ÅRIOS ====================
    
    utils_group = parser.add_argument_group('üõ†Ô∏è Utilit√°rios')
    utils_group.add_argument('--analyze', metavar='CSV_FILE',
                            help='Analisa arquivo de crawl existente')
    utils_group.add_argument('--stats-only', action='store_true',
                            help='Mostra apenas estat√≠sticas, n√£o exporta arquivo')
    utils_group.add_argument('--version', action='version', version='SEOFrog v0.2.0')
    utils_group.add_argument('--dry-run', action='store_true',
                            help='Simula execu√ß√£o sem fazer crawl real')
    
    return parser

# ==================== CONFIG BUILDER ====================

def build_config_from_args(args) -> Dict[str, Any]:
    """Constr√≥i configura√ß√£o final a partir dos argumentos CLI"""
    
    config_dict = {}
    
    # === APLICA PROFILE SE ESPECIFICADO ===
    if args.profile:
        try:
            profile_config = create_config_from_profile(args.profile)
            config_dict = profile_config.to_dict()
            print(f"üìã Usando profile '{args.profile}': {ProfileConfig.get_profile(args.profile).description}")
        except ValueError as e:
            raise ConfigException(f"Profile inv√°lido: {e}")
    else:
        # Usa configura√ß√£o autom√°tica
        auto_config = create_auto_config()
        config_dict = auto_config.to_dict()
        print(f"ü§ñ Configura√ß√£o autom√°tica: {auto_config.max_workers} workers, {auto_config.max_urls:,} URLs max")
    
    # === APLICA OVERRIDES DOS ARGUMENTOS ===
    
    # Mapeamento de argumentos CLI para config
    arg_mappings = {
        'max_urls': 'max_urls',
        'max_depth': 'max_depth', 
        'workers': 'max_workers',
        'delay': 'delay',
        'timeout': 'timeout',
        'max_redirects': 'max_redirects',
        'user_agent': 'user_agent',
        'retry_attempts': 'retry_attempts',
        'output': 'output_dir',
        'format': 'export_format',
        'log_level': 'log_level',
        'memory_limit': 'memory_limit_mb'
    }
    
    # Aplica overrides espec√≠ficos
    for arg_name, config_key in arg_mappings.items():
        if hasattr(args, arg_name) and getattr(args, arg_name) is not None:
            config_dict[config_key] = getattr(args, arg_name)
    
    # === APLICA FLAGS BOOLEANAS ===
    
    if args.no_robots:
        config_dict['respect_robots'] = False
    if args.no_redirects:
        config_dict['follow_redirects'] = False
    if args.crawl_images:
        config_dict['crawl_images'] = True
    if args.crawl_css:
        config_dict['crawl_css'] = True
    if args.crawl_js:
        config_dict['crawl_js'] = True
    if args.crawl_pdf:
        config_dict['crawl_pdf'] = True
    
    return config_dict

# ==================== VALIDA√á√ïES ====================

def validate_final_config(config_dict: Dict[str, Any]) -> None:
    """Valida configura√ß√£o final antes de usar"""
    
    # Cria CrawlConfig tempor√°rio para valida√ß√£o
    try:
        temp_config = CrawlConfig(**config_dict)
        temp_config.validate()  # Usa valida√ß√£o interna
    except (TypeError, ValueError) as e:
        raise ConfigException(f"Configura√ß√£o inv√°lida: {e}")

# ==================== DISPLAY FUNCTIONS ====================

def show_profiles():
    """Mostra detalhes de todos os profiles"""
    profiles = ProfileConfig.get_profiles()
    
    print("\nüöÄ PROFILES ENTERPRISE DISPON√çVEIS:")
    print("=" * 60)
    
    for name, profile in profiles.items():
        config = profile.config
        print(f"\nüìã {name.upper()}")
        print(f"   üìÑ {profile.description}")
        print(f"   üî¢ Max URLs: {config.max_urls:,}")
        print(f"   üìä Max Depth: {config.max_depth}")
        print(f"   ‚ö° Workers: {config.max_workers}")
        print(f"   ‚è±Ô∏è  Delay: {config.delay}s")
        print(f"   ‚è∞ Timeout: {config.timeout}s")
        print(f"   ü§ñ Robots: {'Respeita' if config.respect_robots else 'Ignora'}")
        
        content_types = []
        if config.crawl_images: content_types.append('Images')
        if config.crawl_css: content_types.append('CSS')
        if config.crawl_js: content_types.append('JS')
        
        print(f"   üìÅ Conte√∫do: HTML" + (f" + {', '.join(content_types)}" if content_types else " apenas"))

def show_config_summary(url: str, config_dict: Dict[str, Any]):
    """Mostra resumo da configura√ß√£o enterprise"""
    print(f"\nüöÄ SEOFrog v0.2 Enterprise - CONFIGURA√á√ÉO ATIVA")
    print("=" * 60)
    print(f"üéØ URL de destino: {url}")
    print(f"üìä Max URLs: {config_dict.get('max_urls', 'N/A'):,}")
    print(f"üîç Max Depth: {config_dict.get('max_depth', 'N/A')}")
    print(f"‚ö° Workers: {config_dict.get('max_workers', 'N/A')}")
    print(f"‚è±Ô∏è  Delay: {config_dict.get('delay', 'N/A')}s")
    print(f"‚è∞ Timeout: {config_dict.get('timeout', 'N/A')}s")
    print(f"üîÑ Max Redirects: {config_dict.get('max_redirects', 'N/A')}")
    print(f"ü§ñ Robots.txt: {'Respeita' if config_dict.get('respect_robots', True) else 'Ignora'}")
    print(f"üîÑ Redirects: {'Segue' if config_dict.get('follow_redirects', True) else 'Bloqueia'}")
    print(f"üîÅ Retry attempts: {config_dict.get('retry_attempts', 'N/A')}")
    
    # Tipos de conte√∫do
    crawl_types = []
    if config_dict.get('crawl_images'): crawl_types.append('Images')
    if config_dict.get('crawl_css'): crawl_types.append('CSS')
    if config_dict.get('crawl_js'): crawl_types.append('JS')
    if config_dict.get('crawl_pdf'): crawl_types.append('PDF')
    
    print(f"üìÅ Tipos: HTML" + (f" + {', '.join(crawl_types)}" if crawl_types else " apenas"))
    print(f"üíæ Output: {config_dict.get('output_dir', 'N/A')}/")
    print(f"üìù Log level: {config_dict.get('log_level', 'N/A')}")
    print(f"üíø Formato: {config_dict.get('export_format', 'csv').upper()}")
    
    # Flags especiais
    flags = []
    if config_dict.get('stats_only'): flags.append('Stats Only')
    if config_dict.get('dry_run'): flags.append('Dry Run')
    if flags:
        print(f"üè∑Ô∏è  Flags: {', '.join(flags)}")
    
    print("=" * 60)

# ==================== MAIN CLI FUNCTION ====================

def parse_cli_args() -> Tuple[Optional[str], Dict[str, Any]]:
    """Fun√ß√£o principal do CLI - retorna URL e configura√ß√£o"""
    
    parser = create_cli_parser()
    args = parser.parse_args()
    
    # === UTILIT√ÅRIOS QUE N√ÉO PRECISAM DE URL ===
    
    if args.list_profiles:
        show_profiles()
        sys.exit(0)
    
    if args.analyze:
        return None, {'analyze_file': args.analyze}
    
    # === VALIDA URL OBRIGAT√ìRIA PARA CRAWL ===
    
    if not args.url:
        parser.error("URL √© obrigat√≥ria para crawling. Use --help para exemplos ou --analyze para analisar arquivo existente.")
    
    # === SANITIZA E VALIDA URL ===
    
    try:
        url = sanitize_url(args.url)
    except ValidationException as e:
        parser.error(f"Erro na URL: {e.message}")
    
    # === CONSTR√ìI CONFIGURA√á√ÉO ===
    
    try:
        config_dict = build_config_from_args(args)
        validate_final_config(config_dict)
    except (ConfigException, ValidationException) as e:
        parser.error(f"Erro na configura√ß√£o: {e.message}")
    
    # === ADICIONA FLAGS ESPECIAIS ===
    
    config_dict['stats_only'] = args.stats_only
    config_dict['dry_run'] = args.dry_run
    config_dict['filename'] = args.filename
    
    # === MOSTRA RESUMO DA CONFIGURA√á√ÉO ===
    
    if not args.dry_run:  # N√£o mostra resumo no dry-run
        show_config_summary(url, config_dict)
    
    return url, config_dict

# ==================== INTEGRATION HELPER ====================

def get_config_for_seofrog(config_dict: Dict[str, Any]) -> CrawlConfig:
    """Converte dict de config para CrawlConfig dataclass"""
    
    try:
        # Remove argumentos que n√£o s√£o parte do CrawlConfig
        crawl_config_keys = {
            'max_urls', 'max_depth', 'delay', 'timeout', 'max_workers', 'max_redirects',
            'respect_robots', 'follow_redirects', 'crawl_images', 'crawl_css', 'crawl_js', 'crawl_pdf',
            'user_agent', 'retry_attempts', 'retry_backoff', 'connection_pool_size',
            'output_dir', 'log_level', 'export_format', 'memory_limit_mb', 'enable_compression',
            'chunk_size', 'custom_headers', 'ignore_extensions'
        }
        
        # Filtra apenas as chaves v√°lidas para CrawlConfig
        filtered_config = {k: v for k, v in config_dict.items() if k in crawl_config_keys}
        
        return CrawlConfig(**filtered_config)
    except (TypeError, ValueError) as e:
        raise ConfigException(f"Erro criando CrawlConfig: {e}")

# ==================== STANDALONE TESTING ====================

if __name__ == "__main__":
    """Teste standalone do CLI"""
    try:
        url, config = parse_cli_args()
        
        if config.get('analyze_file'):
            print(f"üîç Modo an√°lise: {config['analyze_file']}")
        else:
            print(f"üöÄ Modo crawl: {url}")
            print(f"üìä Config final: {len(config)} par√¢metros configurados")
            
            # Testa convers√£o para CrawlConfig
            crawl_config = get_config_for_seofrog(config)
            print(f"‚úÖ CrawlConfig criado com sucesso: {crawl_config.max_workers} workers")
            
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        sys.exit(1)